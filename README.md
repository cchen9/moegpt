# moegpt
Mixture of Experts (MoE) implementation for minGPT by Val Krigan

My contribution here is only MoE implementations plus some necessary changes to hook it up.
It support training in soft MoE and sparce modes. First one is used to pretrain router.


The rest of the code was borrowed from here:


https://github.com/karpathy/minGPT
